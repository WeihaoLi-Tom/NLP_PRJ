{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 2025 COMP90042 Project\n",
    "*Make sure you change the file name with your group id.*"
   ],
   "metadata": {
    "id": "32yCsRUo8H33"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T06:01:32.012464Z",
     "start_time": "2025-04-27T06:01:32.009940Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Readme\n",
    "*If there is something to be noted for the marker, please mention here.*\n",
    "\n",
    "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
   ],
   "metadata": {
    "id": "XCybYoGz8YWQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.DataSet Processing\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ],
   "metadata": {
    "id": "6po98qVA8bJD"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T06:01:34.630595Z",
     "start_time": "2025-04-27T06:01:32.045396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T06:01:35.538060Z",
     "start_time": "2025-04-27T06:01:34.869380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "login(\"运行这个之前改成你们自己的token，git不准我上传我的\")\n",
    "                \n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DATA COLLECTION"
  },
  {
   "cell_type": "code",
   "source": [
    "import os, json, pandas as pd\n",
    "\n",
    "\n",
    "DATA_DIR   = \"./data\"\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"train-claims.json\")\n",
    "DEV_FILE   = os.path.join(DATA_DIR, \"dev-claims.json\")\n",
    "TEST_FILE  = os.path.join(DATA_DIR, \"test-claims-unlabelled.json\")\n",
    "EVID_FILE  = os.path.join(DATA_DIR, \"evidence.json\")\n",
    "\n",
    "\n",
    "def load_claims(path: str, labelled: bool = True) -> pd.DataFrame:\n",
    "   \n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "\n",
    "    rows = []\n",
    "    for cid, info in raw.items():\n",
    "        row = {\n",
    "            \"claim_id\":   cid,\n",
    "            \"claim_text\": info.get(\"claim_text\", \"\")\n",
    "        }\n",
    "        if labelled:\n",
    "            row[\"label\"]      = info[\"claim_label\"]\n",
    "            row[\"evid_ids\"]   = info[\"evidences\"]     # list[str]\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "   \n",
    "    if labelled:\n",
    "        df[\"label\"] = df[\"label\"].astype(\"category\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_evidence(path: str):\n",
    "    \n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "\n",
    "    df   = pd.DataFrame([{\"evid_id\": k, \"evid_text\": v} for k, v in raw.items()])\n",
    "    edict = {k: v for k, v in raw.items()}\n",
    "    return df, edict\n",
    "\n",
    "\n",
    "\n",
    "df_train = load_claims(TRAIN_FILE, labelled=True)\n",
    "df_dev   = load_claims(DEV_FILE,   labelled=True)\n",
    "df_test  = load_claims(TEST_FILE,  labelled=False)\n",
    "\n",
    "df_evid, evid_dict = load_evidence(EVID_FILE)    \n",
    "\n",
    "\n",
    "LABEL2ID = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT_ENOUGH_INFO\": 2, \"DISPUTED\": 3}\n",
    "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "for df in (df_train, df_dev):\n",
    "    df[\"label_id\"] = df[\"label\"].map(LABEL2ID).astype(\"int8\")\n",
    "\n",
    "print(f\"Train size: {len(df_train):,}\")\n",
    "print(f\"Dev   size: {len(df_dev):,}\")\n",
    "print(f\"Test  size: {len(df_test):,}\")\n",
    "print(f\"Evidence passages: {len(df_evid):,}\")\n",
    "\n",
    "display(df_train.head())\n",
    "display(df_evid.head())\n",
    "\n",
    "print(\"Train label distribution:\")\n",
    "display(df_train[\"label\"].value_counts())\n",
    "\n",
    "print(\"Dev label distribution:\")\n",
    "display(df_dev[\"label\"].value_counts())\n"
   ],
   "metadata": {
    "id": "qvff21Hv8zjk",
    "ExecuteTime": {
     "end_time": "2025-04-27T06:01:37.125487Z",
     "start_time": "2025-04-27T06:01:35.553030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1,228\n",
      "Dev   size: 154\n",
      "Test  size: 153\n",
      "Evidence passages: 1,208,827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     claim_id                                         claim_text  \\\n",
       "0  claim-1937  Not only is there no scientific evidence that ...   \n",
       "1   claim-126  El Niño drove record highs in global temperatu...   \n",
       "2  claim-2510             In 1946, PDO switched to a cool phase.   \n",
       "3  claim-2021  Weather Channel co-founder John Coleman provid...   \n",
       "4  claim-2449  \"January 2008 capped a 12 month period of glob...   \n",
       "\n",
       "             label                                           evid_ids  \\\n",
       "0         DISPUTED  [evidence-442946, evidence-1194317, evidence-1...   \n",
       "1          REFUTES                [evidence-338219, evidence-1127398]   \n",
       "2         SUPPORTS                 [evidence-530063, evidence-984887]   \n",
       "3         DISPUTED  [evidence-1177431, evidence-782448, evidence-5...   \n",
       "4  NOT_ENOUGH_INFO  [evidence-1010750, evidence-91661, evidence-72...   \n",
       "\n",
       "   label_id  \n",
       "0         3  \n",
       "1         1  \n",
       "2         0  \n",
       "3         3  \n",
       "4         2  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>claim_text</th>\n",
       "      <th>label</th>\n",
       "      <th>evid_ids</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claim-1937</td>\n",
       "      <td>Not only is there no scientific evidence that ...</td>\n",
       "      <td>DISPUTED</td>\n",
       "      <td>[evidence-442946, evidence-1194317, evidence-1...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>claim-126</td>\n",
       "      <td>El Niño drove record highs in global temperatu...</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[evidence-338219, evidence-1127398]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claim-2510</td>\n",
       "      <td>In 1946, PDO switched to a cool phase.</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[evidence-530063, evidence-984887]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claim-2021</td>\n",
       "      <td>Weather Channel co-founder John Coleman provid...</td>\n",
       "      <td>DISPUTED</td>\n",
       "      <td>[evidence-1177431, evidence-782448, evidence-5...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>claim-2449</td>\n",
       "      <td>\"January 2008 capped a 12 month period of glob...</td>\n",
       "      <td>NOT_ENOUGH_INFO</td>\n",
       "      <td>[evidence-1010750, evidence-91661, evidence-72...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "      evid_id                                          evid_text\n",
       "0  evidence-0  John Bennet Lawes, English entrepreneur and ag...\n",
       "1  evidence-1  Lindberg began his professional career at the ...\n",
       "2  evidence-2  ``Boston (Ladies of Cambridge)'' by Vampire We...\n",
       "3  evidence-3  Gerald Francis Goyer (born October 20, 1936) w...\n",
       "4  evidence-4  He detected abnormalities of oxytocinergic fun..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evid_id</th>\n",
       "      <th>evid_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>evidence-0</td>\n",
       "      <td>John Bennet Lawes, English entrepreneur and ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evidence-1</td>\n",
       "      <td>Lindberg began his professional career at the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>evidence-2</td>\n",
       "      <td>``Boston (Ladies of Cambridge)'' by Vampire We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>evidence-3</td>\n",
       "      <td>Gerald Francis Goyer (born October 20, 1936) w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>evidence-4</td>\n",
       "      <td>He detected abnormalities of oxytocinergic fun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "SUPPORTS           519\n",
       "NOT_ENOUGH_INFO    386\n",
       "REFUTES            199\n",
       "DISPUTED           124\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev label distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "SUPPORTS           68\n",
       "NOT_ENOUGH_INFO    41\n",
       "REFUTES            27\n",
       "DISPUTED           18\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 预处理\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T06:01:49.404489Z",
     "start_time": "2025-04-27T06:01:37.412469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install -q rank_bm25\n",
    "!pip install -q tqdm\n",
    "!pip install -q sentence-transformers hnswlib\n",
    "!pip install -U bitsandbytes transformers accelerate"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for hnswlib (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [5 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'hnswlib' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for hnswlib\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (hnswlib)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.45.5)\n",
      "Requirement already satisfied: transformers in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: torch<3,>=2.0 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bitsandbytes) (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tom\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.5)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T06:01:49.479738Z",
     "start_time": "2025-04-27T06:01:49.475053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLIMATE_CONCEPTS = {\n",
    "    \"physical_mechanisms\": [\n",
    "        \"greenhouse effect\", \"carbon dioxide\", \"CO2\", \"methane\", \"CH4\", \n",
    "        \"greenhouse gas\", \"GHG\", \"emissions\", \"fossil fuel\", \"carbon cycle\",\n",
    "        \"radiative forcing\", \"albedo\", \"feedback\", \"sensitivity\"\n",
    "    ],\n",
    "    \"observations\": [\n",
    "        \"temperature\", \"warming\", \"cooling\", \"precipitation\", \"sea level\", \n",
    "        \"ice sheet\", \"glacier\", \"sea ice\", \"ocean acidification\", \"drought\",\n",
    "        \"flood\", \"extreme weather\", \"heat wave\", \"storm\", \"hurricane\"\n",
    "    ],\n",
    "    \"climate_systems\": [\n",
    "        \"atmosphere\", \"ocean\", \"cryosphere\", \"biosphere\", \"El Niño\", \"La Niña\", \n",
    "        \"jet stream\", \"gulf stream\", \"AMOC\", \"PDO\", \"AMO\", \"ENSO\", \"monsoon\"\n",
    "    ],\n",
    "    \"time_periods\": [\n",
    "        \"pre-industrial\", \"industrial\", \"holocene\", \"anthropocene\", \"pleistocene\",\n",
    "        \"ice age\", \"medieval warm period\", \"little ice age\", \"paleoclimate\"\n",
    "    ]\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T06:01:49.490359Z",
     "start_time": "2025-04-27T06:01:49.483830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import hnswlib\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T06:01:49.548298Z",
     "start_time": "2025-04-27T06:01:49.545690Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 粗提取-1\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:26:48.660255Z",
     "start_time": "2025-04-27T11:26:18.929451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import pathlib, gzip, pickle, json, numpy as np, hnswlib\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "K_BM25   = 600    \n",
    "N_DENSE  = 600    \n",
    "EF_QUERY = 200    \n",
    "\n",
    "def tokenize(text: str):\n",
    "    return [tok.lower() for tok in wordpunct_tokenize(text) if tok.isalnum()]\n",
    "\n",
    "\n",
    "corpus_tokens = [tokenize(t) for t in evid_dict.values()]\n",
    "bm25          = BM25Okapi(corpus_tokens)\n",
    "evid_list     = list(evid_dict.keys())         \n",
    "\n",
    "\n",
    "def get_bm25_topk(df, K=K_BM25):\n",
    "    cache_path = pathlib.Path(f\"bm25_top{K}.pkl.gz\")\n",
    "    if cache_path.exists():\n",
    "        with gzip.open(cache_path, \"rb\") as f:\n",
    "            print(f\"✓ load BM25 cache ({cache_path})\")\n",
    "            return pickle.load(f)\n",
    "\n",
    "    claim_topk = {}\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"BM25 Top-{K}\"):\n",
    "        idxs = np.argsort(bm25.get_scores(tokenize(row.claim_text)))[::-1][:K]\n",
    "        claim_topk[row.claim_id] = [evid_list[i] for i in idxs]\n",
    "\n",
    "    with gzip.open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(claim_topk, f)\n",
    "    print(f\"⌛ save cache to {cache_path}\")\n",
    "    return claim_topk\n",
    "\n",
    "                 \n",
    "bm25_topk = get_bm25_topk(df_dev, K_BM25)\n",
    "\n",
    "# MiniLM + HNSW \n",
    "MODEL = \"all-MiniLM-L6-v2\"\n",
    "minilm = SentenceTransformer(MODEL)\n",
    "DIM    = minilm.get_sentence_embedding_dimension()\n",
    "\n",
    "hnsw_bin = pathlib.Path(\"evidence_hnsw.bin\")\n",
    "ids_npy  = pathlib.Path(\"evid_ids.npy\")\n",
    "evid_ids = np.load(ids_npy)\n",
    "if not hnsw_bin.exists():\n",
    "    \n",
    "    ev_emb = minilm.encode(list(evid_dict.values()),\n",
    "                           batch_size=256,\n",
    "                           normalize_embeddings=True,\n",
    "                           show_progress_bar=True)\n",
    "    np.save(ids_npy, np.array(evid_list))\n",
    "\n",
    "    \n",
    "    idx = hnswlib.Index(space=\"cosine\", dim=DIM)\n",
    "    idx.init_index(max_elements=len(ev_emb), ef_construction=200, M=32)\n",
    "    idx.add_items(ev_emb)\n",
    "    idx.save_index(hnsw_bin)\n",
    "else:\n",
    "    idx = hnswlib.Index(space=\"cosine\", dim=DIM)\n",
    "    idx.load_index(str(hnsw_bin), max_elements=len(evid_ids))\n",
    "\n",
    "\n",
    "idx.set_ef(EF_QUERY)                 \n",
    "\n",
    "def dense_retrieve(text, k=N_DENSE):\n",
    "    q_vec = minilm.encode(text, normalize_embeddings=True)\n",
    "    I, _ = idx.knn_query(q_vec, k=k)\n",
    "    return [evid_ids[i] for i in I[0]]\n",
    "\n",
    "\n",
    "claim_topk = {}\n",
    "for _, row in tqdm(df_dev.iterrows(), total=len(df_dev),\n",
    "                   desc=\"BM25∪HNSW retrieval\"):\n",
    "    merged = list(dict.fromkeys(\n",
    "        bm25_topk[row.claim_id] + dense_retrieve(row.claim_text)))\n",
    "    claim_topk[row.claim_id] = merged\n",
    "\n",
    "\n",
    "\n",
    "def recall_at_k(dev_df, topk_map):\n",
    "    rs = []\n",
    "    for _, r in dev_df.iterrows():\n",
    "        gold  = set(r.evid_ids)\n",
    "        found = len(gold & set(topk_map[r.claim_id]))\n",
    "        rs.append(found / len(gold))\n",
    "    return sum(rs) / len(rs)\n",
    "\n",
    "recall_val = recall_at_k(df_dev, claim_topk)\n",
    "print(f\"\\nRecall(BM25∪HNSW) on dev: {recall_val:.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ load BM25 cache (bm25_top600.pkl.gz)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BM25∪HNSW retrieval: 100%|██████████| 154/154 [00:01<00:00, 110.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recall(BM25∪HNSW) on dev: 0.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 粗提取-2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 这个是第一版本，recall大概是0.69\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    return [w for w in words if len(w) > 3] \n",
    "\n",
    "def calculate_recall(gold_evidence_ids, predicted_evidence_ids):\n",
    "   \n",
    "    if not gold_evidence_ids:\n",
    "        return 1.0 \n",
    "    \n",
    "    found = len(set(gold_evidence_ids) & set(predicted_evidence_ids))\n",
    "    return found / len(gold_evidence_ids)\n",
    "\n",
    "def improved_coarse_retrieval(df, evid_dict, bm25, evid_list, minilm, idx, evid_ids, \n",
    "                              candidates_per_claim=200, cache_path=\"improved_coarse_candidates.pkl.gz\"):\n",
    "    \n",
    "    \n",
    "    \n",
    "    cache_file = pathlib.Path(cache_path)\n",
    "    if cache_file.exists():\n",
    "        with gzip.open(cache_file, \"rb\") as f:\n",
    "            print(f\"✓ load cache ({cache_file})\")\n",
    "            cached_data = pickle.load(f)\n",
    "            return cached_data['candidates'], cached_data['recall']\n",
    "    \n",
    "    claim_candidates = {}\n",
    "    recall_values = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"improved \"):\n",
    "        claim_id = row['claim_id']\n",
    "        claim_text = row['claim_text']\n",
    "        \n",
    "       \n",
    "        gold_evidence = row.get('evid_ids', [])\n",
    "        \n",
    "        \n",
    "        claim_keywords = set(simple_tokenize(claim_text))\n",
    "        keyword_candidates = []\n",
    "        \n",
    "        if len(claim_keywords) > 0:\n",
    "            \n",
    "            sample_size = min(10000, len(evid_dict))\n",
    "            sample_eids = list(evid_dict.keys())[:sample_size]\n",
    "            \n",
    "            for eid in sample_eids:\n",
    "                \n",
    "                evid_sample = evid_dict[eid][:200]\n",
    "                evid_words = set(simple_tokenize(evid_sample))\n",
    "                overlap = len(claim_keywords & evid_words)\n",
    "                \n",
    "               \n",
    "                if overlap >= min(2, len(claim_keywords) // 3):\n",
    "                    keyword_candidates.append(eid)\n",
    "            \n",
    "            keyword_candidates = keyword_candidates[:500]  \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            \n",
    "            bm25_scores = bm25.get_scores([w.lower() for w in re.findall(r'\\w+', claim_text) if len(w) > 2])\n",
    "            top_bm25_indices = np.argsort(bm25_scores)[::-1][:300]\n",
    "            bm25_candidates = [evid_list[i] for i in top_bm25_indices]\n",
    "        except Exception as e:\n",
    "            print(f\"BM25 error: {e}\")\n",
    "            bm25_candidates = []\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            q_embedding = minilm.encode(claim_text, normalize_embeddings=True)\n",
    "            I, _ = idx.knn_query(q_embedding.reshape(1, -1), k=300)\n",
    "            dense_candidates = [evid_ids[i] for i in I[0]]\n",
    "        except Exception as e:\n",
    "            print(f\"dense error: {e}\")\n",
    "            dense_candidates = []\n",
    "        \n",
    "        \n",
    "        merged_candidates = []\n",
    "        \n",
    "        \n",
    "        bm25_set = set(bm25_candidates[:100])\n",
    "        dense_set = set(dense_candidates[:100])\n",
    "        high_confidence = list(bm25_set.intersection(dense_set))\n",
    "        merged_candidates.extend(high_confidence)\n",
    "        \n",
    "        \n",
    "        for i in range(min(75, len(bm25_candidates))):\n",
    "            if bm25_candidates[i] not in merged_candidates:\n",
    "                merged_candidates.append(bm25_candidates[i])\n",
    "        \n",
    "        for i in range(min(75, len(dense_candidates))):\n",
    "            if dense_candidates[i] not in merged_candidates:\n",
    "                merged_candidates.append(dense_candidates[i])\n",
    "        \n",
    "        \n",
    "        if keyword_candidates:\n",
    "            keyword_set = set(keyword_candidates)\n",
    "            for candidate in list(keyword_set - set(merged_candidates))[:50]:\n",
    "                merged_candidates.append(candidate)\n",
    "        \n",
    "        \n",
    "        remaining = list(set(bm25_candidates + dense_candidates) - set(merged_candidates))\n",
    "        merged_candidates.extend(remaining[:max(0, candidates_per_claim - len(merged_candidates))])\n",
    "        \n",
    "        \n",
    "        final_candidates = merged_candidates[:candidates_per_claim]\n",
    "        claim_candidates[claim_id] = final_candidates\n",
    "        \n",
    "        \n",
    "        if hasattr(row, 'evid_ids') and row.evid_ids:\n",
    "            recall = calculate_recall(row.evid_ids, final_candidates)\n",
    "            recall_values.append(recall)\n",
    "    \n",
    "    \n",
    "    avg_recall = sum(recall_values) / len(recall_values) if recall_values else 0\n",
    "    \n",
    "    \n",
    "    with gzip.open(cache_file, \"wb\") as f:\n",
    "        cache_data = {\n",
    "            'candidates': claim_candidates,\n",
    "            'recall': avg_recall\n",
    "        }\n",
    "        pickle.dump(cache_data, f)\n",
    "    print(f\"⌛ save cache {cache_file}\")\n",
    "    \n",
    "    print(f\"\\naverage recall: {avg_recall:.4f}\")\n",
    "    print(f\"candicates num: {candidates_per_claim}\")\n",
    "    \n",
    "    \n",
    "    if len(recall_values) > 0:\n",
    "        \n",
    "        for i, (_, row) in enumerate(df.iterrows()):\n",
    "            if hasattr(row, 'evid_ids') and i < 5: \n",
    "                cid = row['claim_id']\n",
    "                gold_count = len(row.evid_ids)\n",
    "                found = len(set(row.evid_ids) & set(claim_candidates[cid]))\n",
    "                \n",
    "    \n",
    "    return claim_candidates, avg_recall\n",
    "\n",
    "\n",
    "improved_candidates, improved_recall = improved_coarse_retrieval(\n",
    "    df_dev,\n",
    "    evid_dict, \n",
    "    bm25, \n",
    "    evid_list, \n",
    "    minilm, \n",
    "    idx, \n",
    "    evid_ids,\n",
    "    candidates_per_claim=200 \n",
    ")\n",
    "\n",
    "print(f\"\\n recall: {improved_recall:.4f}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:24:40.700518Z",
     "start_time": "2025-04-27T11:24:40.660536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 这个是改进的粗提取模块2.0版本\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import pathlib, gzip, pickle\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hnswlib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    return [w for w in words if len(w) > 3]\n",
    "\n",
    "def calculate_recall(gold_ids, pred_ids):\n",
    "    \n",
    "    if not gold_ids:\n",
    "        return 1.0\n",
    "    return len(set(gold_ids) & set(pred_ids)) / len(gold_ids)\n",
    "\n",
    "def improved_coarse_retrieval(df,\n",
    "                              evid_dict,\n",
    "                              bm25: BM25Okapi,\n",
    "                              evid_list,\n",
    "                              sbert: SentenceTransformer,\n",
    "                              hnsw_index: hnswlib.Index,\n",
    "                              evid_ids,\n",
    "                              k_bm25=100,\n",
    "                              k_dense=100,\n",
    "                              rrf_k=60,\n",
    "                              mmr_lambda=0.7,\n",
    "                              candidates_per_claim=200,\n",
    "                              cache_path=\"improved_coarse_candidates.pkl.gz\"):\n",
    "\n",
    "    cache_file = pathlib.Path(cache_path)\n",
    "    if cache_file.exists():\n",
    "        with gzip.open(cache_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"✓ load from cache: {cache_file}\")\n",
    "        return data['candidates'], data['recall']\n",
    "    \n",
    "    claim_candidates = {}\n",
    "    recall_vals = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"improved\"):\n",
    "        cid = row['claim_id']\n",
    "        text = row['claim_text']\n",
    "        gold = row.get('evid_ids', [])\n",
    "        \n",
    "        # 1. BM25\n",
    "        tokens = simple_tokenize(text)\n",
    "        bm25_scores = bm25.get_scores(tokens)\n",
    "        top_bm25 = np.argsort(-bm25_scores)[:k_bm25]\n",
    "        bm25_ids = [evid_list[i] for i in top_bm25]\n",
    "        bm25_rank = {evid_list[i]: r for r,i in enumerate(top_bm25)}\n",
    "        \n",
    "        # 2. Dense \n",
    "        q_emb = sbert.encode(text, normalize_embeddings=True)\n",
    "        labels, _ = hnsw_index.knn_query(q_emb.reshape(1, -1), k=k_dense)\n",
    "        dense_idx = labels[0]\n",
    "        dense_ids = [evid_ids[i] for i in dense_idx]\n",
    "        dense_rank = {evid_ids[i]: r for r,i in enumerate(dense_idx)}\n",
    "        \n",
    "        # 3. RRF\n",
    "        fusion = {}\n",
    "        for eid in set(bm25_ids + dense_ids):\n",
    "            score = 0.0\n",
    "            if eid in bm25_rank:\n",
    "                score += 1/(bm25_rank[eid] + rrf_k)\n",
    "            if eid in dense_rank:\n",
    "                score += 1/(dense_rank[eid] + rrf_k)\n",
    "            fusion[eid] = score\n",
    "        fused = [eid for eid,_ in sorted(fusion.items(), key=lambda x: -x[1])]\n",
    "        \n",
    "        # 4. MMR\n",
    "        pool = fused[:2*candidates_per_claim]\n",
    "        pool_embs = sbert.encode([evid_dict[e] for e in pool], normalize_embeddings=True)\n",
    "        sim_q = cosine_similarity(q_emb.reshape(1,-1), pool_embs)[0]\n",
    "        \n",
    "        selected, cand_ids, cand_embs = [], pool.copy(), pool_embs.copy()\n",
    "        sim_to_q = sim_q.copy()\n",
    "        while len(selected) < candidates_per_claim and cand_ids:\n",
    "            if not selected:\n",
    "                idx0 = int(np.argmax(sim_to_q))\n",
    "            else:\n",
    "                sims = cosine_similarity(q_emb.reshape(1,-1), cand_embs)[0]\n",
    "                sel_embs = sbert.encode([evid_dict[e] for e in selected], normalize_embeddings=True)\n",
    "                sim_sel = cosine_similarity(cand_embs, sel_embs)\n",
    "                max_sel = np.max(sim_sel, axis=1)\n",
    "                mmr_scores = mmr_lambda * sims - (1-mmr_lambda) * max_sel\n",
    "                idx0 = int(np.argmax(mmr_scores))\n",
    "            selected.append(cand_ids.pop(idx0))\n",
    "            cand_embs = np.delete(cand_embs, idx0, axis=0)\n",
    "            sim_to_q = np.delete(sim_to_q, idx0)\n",
    "        \n",
    "        claim_candidates[cid] = selected\n",
    "        recall_vals.append(calculate_recall(gold, selected))\n",
    "    \n",
    "    mean_rec = float(np.mean(recall_vals))\n",
    "    \n",
    "    with gzip.open(cache_file, \"wb\") as f:\n",
    "        pickle.dump({'candidates': claim_candidates, 'recall': mean_rec}, f)\n",
    "    print(f\"⌛ cache: {cache_file}\")\n",
    "    print(f\"average recall : {mean_rec:.4f}  |  candicate: {candidates_per_claim}\")\n",
    "    \n",
    "   \n",
    "    print(\"\\n sample:\")\n",
    "    for i, row in df.head(3).iterrows():\n",
    "        cid = row['claim_id']\n",
    "        gold = row.get('evid_ids', [])\n",
    "        found = len(set(gold)&set(claim_candidates[cid]))\n",
    "        print(f\"{cid}: {found}/{len(gold)} = {found/len(gold):.2f}\")\n",
    "    \n",
    "    return claim_candidates, mean_rec\n",
    "\n",
    "\n",
    "improved_cands, improved_rec = improved_coarse_retrieval(\n",
    "    df_dev, evid_dict, bm25, evid_list, minilm, idx, evid_ids,\n",
    "    k_bm25=100, k_dense=100, rrf_k=60,\n",
    "    mmr_lambda=0.7, candidates_per_claim=200\n",
    ")\n",
    "print(f\"\\nfinal Recall: {improved_rec:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ load from cache: improved_coarse_candidates.pkl.gz\n",
      "\n",
      "final Recall: 0.7158\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Model Implementation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ],
   "metadata": {
    "id": "1FA2ao2l8hOg"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T06:02:19.015637Z",
     "start_time": "2025-04-27T06:02:18.716196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "    print(f\"Current GPU device: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")"
   ],
   "metadata": {
    "id": "QIEqDDT78q39",
    "ExecuteTime": {
     "end_time": "2025-04-27T06:02:19.035182Z",
     "start_time": "2025-04-27T06:02:19.030512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n",
      "Number of available GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 4070 SUPER\n",
      "Current GPU device: 0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T07:25:41.937975Z",
     "start_time": "2025-04-27T07:25:41.935445Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T07:25:46.240550Z",
     "start_time": "2025-04-27T07:25:46.237968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T07:23:19.314093Z",
     "start_time": "2025-04-27T07:23:19.311064Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T06:32:59.373068Z",
     "start_time": "2025-04-27T06:32:59.370499Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T06:33:00.456056Z",
     "start_time": "2025-04-27T06:33:00.453375Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T06:14:38.139677100Z",
     "start_time": "2025-04-26T13:12:03.605380Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T06:14:38.140183900Z",
     "start_time": "2025-04-26T13:12:03.693933Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T06:14:38.140183900Z",
     "start_time": "2025-04-26T13:12:03.775952Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T06:14:38.141192500Z",
     "start_time": "2025-04-26T13:12:03.860220Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T06:14:38.141702900Z",
     "start_time": "2025-04-26T13:12:03.942847Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.Testing and Evaluation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ],
   "metadata": {
    "id": "EzGuzHPE87Ya"
   }
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "6ZVeNYIH9IaL",
    "ExecuteTime": {
     "end_time": "2025-04-27T06:14:38.141702900Z",
     "start_time": "2025-04-26T13:12:04.026795Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Object Oriented Programming codes here\n",
    "\n",
    "*You can use multiple code snippets. Just add more if needed*"
   ],
   "metadata": {
    "id": "mefSOe8eTmGP"
   }
  }
 ]
}
